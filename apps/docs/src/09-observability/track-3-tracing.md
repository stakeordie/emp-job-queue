# Track 3: Tracing ðŸ”—

**Request Flow Tracking and Performance Analysis**

## Goals

**Primary Goal**: Track request lifecycles and performance across distributed services to provide end-to-end visibility into job processing flows.

**Secondary Goals**:
- Identify performance bottlenecks in distributed workflows
- Correlate failures across service boundaries  
- Measure service dependencies and interaction patterns
- Enable root cause analysis for complex issues
- Support performance optimization decisions

## Technology Stack

### Core Components
- **[OpenTelemetry (OTel)](https://opentelemetry.io/)**: Tracing SDK and auto-instrumentation
- **OTel Collector**: Local trace aggregation and forwarding
- **Trace Context Propagation**: Cross-service correlation standards
- **Span Processors**: Batch and real-time span processing

### External Integrations
- **[Dash0](https://www.dash0.com/)**: Primary distributed tracing platform
- **Alternative Options**: Jaeger, Zipkin, AWS X-Ray, Datadog APM

## What It Captures

### Request Flows
- **HTTP Requests**: API server endpoints, WebSocket connections
- **Job Processing**: Complete job lifecycle from submission to completion
- **Service Interactions**: API â†’ Redis â†’ Worker â†’ ComfyUI flows
- **Database Operations**: Redis operations, connection pooling
- **External Calls**: Model downloads, webhook notifications

### Performance Metrics
- **Latency**: Request duration, queue wait times, processing times
- **Throughput**: Requests per second, job completion rates
- **Error Rates**: Failed requests, timeout events, retry attempts
- **Resource Usage**: Service resource consumption during operations

### Cross-Service Context
- **Trace Propagation**: Parent-child span relationships
- **Correlation IDs**: Link related operations across services
- **Service Topology**: Dynamic service dependency mapping
- **Error Attribution**: Failed spans with full context

## Implementation Details

### OTel Collector Configuration

```yaml
# otel-collector-machine.yaml.template
receivers:
  otlp:
    protocols:
      grpc:
        endpoint: 0.0.0.0:4317
      http:
        endpoint: 0.0.0.0:4318

processors:
  batch:
    timeout: 5s
    send_batch_size: 1024
    send_batch_max_size: 2048
  
  resource:
    attributes:
      - key: machine.id
        value: ${MACHINE_ID}
        action: upsert
      - key: deployment.environment  
        value: ${RAILWAY_ENVIRONMENT}
        action: upsert
      - key: service.region
        value: ${RAILWAY_REGION}
        action: upsert

  memory_limiter:
    limit_mib: 256

exporters:
  otlp:
    endpoint: ${DASH0_OTLP_ENDPOINT}
    headers:
      Authorization: "Bearer ${DASH0_API_TOKEN}"
    compression: gzip

service:
  pipelines:
    traces:
      receivers: [otlp]
      processors: [memory_limiter, resource, batch]
      exporters: [otlp]
  
  extensions: [health_check]
  
extensions:
  health_check:
    endpoint: 0.0.0.0:13133
```

### Application Instrumentation

```typescript
// Tracing setup in API server
import { NodeSDK } from '@opentelemetry/sdk-node';
import { Resource } from '@opentelemetry/resources';
import { SemanticResourceAttributes } from '@opentelemetry/semantic-conventions';
import { OTLPTraceExporter } from '@opentelemetry/exporter-otlp-http';

const sdk = new NodeSDK({
  resource: new Resource({
    [SemanticResourceAttributes.SERVICE_NAME]: 'emp-job-queue-api',
    [SemanticResourceAttributes.SERVICE_VERSION]: process.env.SERVICE_VERSION || '1.0.0',
    [SemanticResourceAttributes.DEPLOYMENT_ENVIRONMENT]: process.env.NODE_ENV || 'development',
  }),
  traceExporter: new OTLPTraceExporter({
    url: process.env.OTEL_EXPORTER_OTLP_ENDPOINT || 'http://localhost:4318/v1/traces',
  }),
});

sdk.start();
```

### Job Processing Instrumentation

```typescript
// Worker job processing with tracing
import { trace, SpanStatusCode } from '@opentelemetry/api';

export class TracedJobProcessor {
  private tracer = trace.getTracer('job-processor');

  async processJob(jobData: JobData, progressCallback: ProgressCallback): Promise<JobResult> {
    const span = this.tracer.startSpan('process-job', {
      attributes: {
        'job.id': jobData.id,
        'job.service': jobData.service_required,
        'job.priority': jobData.priority,
        'worker.id': process.env.WORKER_ID,
        'machine.id': process.env.MACHINE_ID,
      }
    });

    try {
      // Child span for Redis operations
      const redisSpan = this.tracer.startSpan('redis-claim-job', { parent: span });
      await this.claimJobFromRedis(jobData.id);
      redisSpan.setStatus({ code: SpanStatusCode.OK });
      redisSpan.end();

      // Child span for service processing
      const serviceSpan = this.tracer.startSpan('service-process-job', { parent: span });
      serviceSpan.setAttributes({
        'service.type': jobData.service_required,
        'service.endpoint': this.getServiceEndpoint()
      });

      const result = await this.processJobImpl(jobData, progressCallback);
      
      serviceSpan.setAttributes({
        'job.output_size': JSON.stringify(result).length,
        'job.success': result.success
      });
      serviceSpan.setStatus({ code: SpanStatusCode.OK });
      serviceSpan.end();

      // Success attributes on parent span
      span.setAttributes({
        'job.processing_time_ms': result.processing_time_ms,
        'job.output_size': JSON.stringify(result).length
      });
      span.setStatus({ code: SpanStatusCode.OK });

      return result;
    } catch (error) {
      // Error handling with span context
      span.recordException(error);
      span.setStatus({ 
        code: SpanStatusCode.ERROR, 
        message: error.message 
      });
      throw error;
    } finally {
      span.end();
    }
  }
}
```

### Redis Trace Context Propagation

```typescript
// Redis job queue with trace context
import { trace, context, propagation } from '@opentelemetry/api';

export class TracedRedisJobQueue {
  async submitJob(jobData: JobData): Promise<void> {
    const span = trace.getActiveSpan();
    
    // Inject trace context into job data
    const traceContext = {};
    propagation.inject(context.active(), traceContext);
    
    const enrichedJobData = {
      ...jobData,
      trace_context: traceContext,
      submitted_at: Date.now(),
      trace_id: span?.spanContext().traceId
    };

    await this.redis.hmset(`job:${jobData.id}`, enrichedJobData);
    await this.redis.zadd('jobs:pending', jobData.priority, jobData.id);
  }

  async claimJob(workerId: string): Promise<JobData | null> {
    const jobId = await this.redis.zpopmax('jobs:pending');
    if (!jobId) return null;

    const jobData = await this.redis.hgetall(`job:${jobId}`);
    
    // Extract and continue trace context
    if (jobData.trace_context) {
      const parentContext = propagation.extract(context.active(), jobData.trace_context);
      
      // Start child span in extracted context
      return trace.setSpan(parentContext, trace.getActiveSpan()).with(() => {
        const span = trace.getTracer('redis-queue').startSpan('job-claimed', {
          attributes: {
            'job.id': jobId,
            'worker.id': workerId,
            'job.wait_time_ms': Date.now() - parseInt(jobData.submitted_at)
          }
        });
        
        span.end();
        return jobData;
      });
    }
    
    return jobData;
  }
}
```

## Current Status: âœ… Active

### âœ… Implemented Features
- **OTel Collector**: Deployed on all worker machines as sidecar containers
- **Auto-instrumentation**: HTTP, Redis, and database operations automatically traced
- **Manual Instrumentation**: Job processing and service interactions manually instrumented
- **Dash0 Integration**: Traces flowing to centralized Dash0 platform
- **Context Propagation**: Trace context preserved across service boundaries
- **Error Correlation**: Failed operations linked to parent traces

### âœ… Service Coverage
- **API Server**: HTTP endpoints, WebSocket connections, Redis operations
- **Workers**: Job processing, connector operations, health checks  
- **Machine Services**: Component installation, service startup, PM2 operations
- **External Calls**: Model downloads, webhook notifications, health checks

### âœ… Data Flow
```mermaid
graph TB
    A[Client Request] --> B[API Server Span]
    B --> C[Redis Operation Span]
    C --> D[Job Queue Span]
    D --> E[Worker Claim Span]
    E --> F[Job Processing Span]
    F --> G[Service Call Span]
    G --> H[Model Download Span]
    
    I[OTel Collector] --> J[Dash0 Platform]
    
    B --> I
    C --> I
    D --> I
    E --> I
    F --> I
    G --> I
    H --> I
    
    J --> K[Trace Visualization]
    J --> L[Performance Analysis]  
    J --> M[Error Attribution]
```

### âœ… Trace Context Flow
- **Request ID Generation**: Unique trace IDs for each client request
- **Context Injection**: Trace context stored in Redis job data
- **Context Extraction**: Workers extract and continue parent traces
- **Span Relationships**: Parent-child relationships preserved across services
- **Error Propagation**: Failed child spans mark parent spans as errors

## Configuration Examples

### Environment Variables
```bash
# OTel configuration
OTEL_SERVICE_NAME=emp-job-queue-api
OTEL_SERVICE_VERSION=1.0.0
OTEL_EXPORTER_OTLP_ENDPOINT=http://localhost:4318
OTEL_RESOURCE_ATTRIBUTES=service.name=emp-job-queue,deployment.environment=production

# Dash0 integration
DASH0_OTLP_ENDPOINT=https://ingress.dash0.com/v1/traces
DASH0_API_TOKEN=${DASH0_API_TOKEN}

# Machine context
MACHINE_ID=machine-prod-01
WORKER_ID=worker-gpu-01
RAILWAY_ENVIRONMENT=production
RAILWAY_REGION=us-west1

# Feature flags
OTEL_TRACES_SAMPLER=traceidratio
OTEL_TRACES_SAMPLER_ARG=0.1  # 10% sampling
```

### Docker Integration
```yaml
# docker-compose.yml
services:
  worker:
    environment:
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://otel-collector:4318
      - OTEL_SERVICE_NAME=emp-worker
      - MACHINE_ID=${MACHINE_ID}
      - WORKER_ID=${WORKER_ID}
    depends_on:
      - otel-collector

  otel-collector:
    image: otel/opentelemetry-collector-contrib:latest
    command: ["--config=/etc/otel-collector-config.yaml"]
    volumes:
      - ./otel-collector-machine.yaml:/etc/otel-collector-config.yaml:ro
    environment:
      - MACHINE_ID=${MACHINE_ID}
      - DASH0_OTLP_ENDPOINT=${DASH0_OTLP_ENDPOINT}
      - DASH0_API_TOKEN=${DASH0_API_TOKEN}
    ports:
      - "4317:4317"  # OTLP gRPC
      - "4318:4318"  # OTLP HTTP
      - "13133:13133"  # Health check
```

## Performance Metrics

### Current Performance
- **Trace Volume**: ~500 traces/minute during peak load
- **Latency Overhead**: <5ms added latency per request
- **Resource Usage**: <100MB RAM, <10% CPU for OTel Collector
- **Storage Impact**: ~50MB/day compressed traces
- **Sampling Rate**: 10% for normal operations, 100% for errors

### Trace Statistics
- **Average Trace Duration**: 2.5 seconds for typical job processing
- **Max Trace Depth**: 8 levels for complex ComfyUI workflows
- **Error Rate**: <1% of traces contain errors
- **Service Coverage**: 95% of operations instrumented

## Debugging and Troubleshooting

### Common Issues

**Problem**: Missing traces in Dash0
```bash
# Check OTel Collector health
curl http://localhost:13133/

# Verify trace export
curl http://localhost:4318/v1/traces -X POST \
  -H "Content-Type: application/json" \
  -d '{"resourceSpans":[{"resource":{"attributes":[]},"scopeSpans":[]}]}'

# Check collector logs
docker logs otel-collector

# Test connectivity to Dash0
curl -X POST ${DASH0_OTLP_ENDPOINT} \
  -H "Authorization: Bearer ${DASH0_API_TOKEN}" \
  -H "Content-Type: application/x-protobuf"
```

**Problem**: Broken trace context
```typescript
// Debug trace propagation
import { trace, context } from '@opentelemetry/api';

function debugTraceContext() {
  const span = trace.getActiveSpan();
  if (span) {
    console.log('Current Trace ID:', span.spanContext().traceId);
    console.log('Current Span ID:', span.spanContext().spanId);
    
    // Check if context is properly propagated
    const traceContext = {};
    propagation.inject(context.active(), traceContext);
    console.log('Injected Context:', traceContext);
  } else {
    console.log('No active span - trace context lost');
  }
}
```

**Problem**: High resource usage
```bash
# Monitor OTel Collector resource usage
docker stats otel-collector --no-stream

# Check batch processing settings
grep -i batch /etc/otel-collector-config.yaml

# Increase batch size to reduce overhead
batch:
  timeout: 10s          # Increase from 5s
  send_batch_size: 2048  # Increase from 1024
  send_batch_max_size: 4096  # Increase from 2048
```

### Health Monitoring
```bash
# OTel Collector health check
curl -f http://localhost:13133/ || echo "OTel Collector unhealthy"

# Check trace export pipeline
curl http://localhost:8888/metrics | grep -i pipeline

# Verify instrumentation
node -e "console.log(require('@opentelemetry/api').trace.getActiveTracer())"
```

## Best Practices

### Span Naming
```typescript
// Good span names - verb-object pattern
span = tracer.startSpan('http-request');
span = tracer.startSpan('redis-get');
span = tracer.startSpan('job-process'); 
span = tracer.startSpan('model-download');

// Bad span names - too generic or detailed
span = tracer.startSpan('operation');  // Too generic
span = tracer.startSpan('GET /api/v1/jobs/123/status');  // Too detailed
```

### Span Attributes
```typescript
// Essential attributes for job processing
span.setAttributes({
  'job.id': jobData.id,
  'job.service': jobData.service_required,
  'job.priority': jobData.priority,
  'worker.id': process.env.WORKER_ID,
  'machine.id': process.env.MACHINE_ID,
  'service.version': '1.0.0',
  'http.method': 'POST',
  'http.status_code': 200,
  'db.operation': 'SELECT',
  'error.type': 'timeout'
});
```

### Error Handling
```typescript
// Proper error recording
try {
  const result = await riskyOperation();
  span.setStatus({ code: SpanStatusCode.OK });
} catch (error) {
  // Record exception with full context
  span.recordException(error);
  span.setStatus({ 
    code: SpanStatusCode.ERROR, 
    message: error.message 
  });
  
  // Add error attributes
  span.setAttributes({
    'error.name': error.name,
    'error.type': classifyError(error),
    'error.recoverable': isRecoverable(error)
  });
  
  throw error;  // Re-throw to maintain error flow
} finally {
  span.end();  // Always end spans
}
```

## Integration with Other Tracks

### Cross-Track Correlation
- **Track 2 (Logs)**: Log entries include trace_id and span_id for correlation
- **Track 4 (Metrics)**: Metrics include trace context for performance correlation
- **Track 5 (Sentry)**: Sentry errors include trace IDs for distributed debugging
- **Track 1 (Operational Event Bus)**: Operational events tagged with trace context

### Correlation Patterns
```typescript
// Example: Correlating trace with logs and metrics
const span = trace.getActiveSpan();
const traceId = span?.spanContext().traceId;

// Log with trace context
logger.info('Job processing started', {
  trace_id: traceId,
  span_id: span?.spanContext().spanId,
  job_id: jobData.id
});

// Metrics with trace context  
metrics.counter('jobs.started').add(1, {
  trace_id: traceId,
  service: jobData.service_required
});

// Sentry error with trace context
Sentry.withScope((scope) => {
  scope.setTag('trace_id', traceId);
  scope.setContext('job', jobData);
  Sentry.captureException(error);
});
```

## Next Steps

### Phase 1: Enhanced Context Propagation (ðŸš§ In Progress)
- **Redis Context Storage**: Store trace context in Redis job data
- **Worker Context Continuation**: Workers extract and continue traces
- **WebSocket Tracing**: Trace real-time WebSocket communications
- **Response Correlation**: Link responses back to originating requests

### Phase 2: Advanced Analysis
- **Service Dependency Mapping**: Automatic service topology discovery
- **Performance Baselines**: Establish normal performance patterns
- **Anomaly Detection**: Identify unusual trace patterns
- **Custom Dashboards**: Service-specific performance dashboards

### Phase 3: Intelligence Layer
- **Predictive Performance**: Predict performance issues from trace patterns  
- **Automated Optimization**: Suggest performance improvements
- **Capacity Planning**: Resource planning from trace analysis
- **Smart Sampling**: Context-aware sampling strategies

## Success Metrics

### Current Achievement (âœ…)
- **95%** of operations instrumented with tracing
- **<5ms** average tracing overhead per request
- **10 second** end-to-end trace visibility
- **99%** trace completeness (no broken spans)

### Target Metrics
- **99%** of operations instrumented
- **<2ms** average tracing overhead
- **Real-time** trace analysis and alerting
- **Automated** performance optimization recommendations
- **Predictive** bottleneck identification