# Basic Machine Logging and Monitoring Guide

## üìç Log Locations

### **Development Mode (Console)**
```bash
# Start with debug logging to see everything
LOG_LEVEL=debug node src/index.js

# Or use the dev script
LOG_LEVEL=debug npm run dev
```

### **Production Mode (Files)**
```bash
# Logs are written to these locations:
/workspace/logs/basic-machine-2025-07-08.log  # All logs
/workspace/logs/error-2025-07-08.log          # Error logs only

# For Docker/testing:
./data/logs/basic-machine-2025-07-08.log      # Mounted volume
```

### **Service-Specific Logs**
```bash
# ComfyUI service logs
/workspace/comfyui_gpu0/logs/output.log       # ComfyUI process output
/workspace/comfyui_gpu1/logs/output.log       # GPU 1, etc.

# Redis Worker logs  
/tmp/worker_gpu0/logs/                        # Worker process logs
```

### **Real-time Log Following**
```bash
# Follow all logs
tail -f /workspace/logs/basic-machine-*.log

# Follow specific service
tail -f /workspace/comfyui_gpu0/logs/output.log

# Docker logs
docker logs -f basic-machine
```

## üöÄ Complete Startup Sequence

```mermaid
flowchart TD
    A[üöÄ Container Start] --> B[üìã Load Environment Config]
    B --> C{üîß Config Valid?}
    C -->|‚ùå No| Z1[üí• Exit with Error]
    C -->|‚úÖ Yes| D[üéØ Create ServiceOrchestrator]
    
    D --> E[üìä Log Startup Info]
    E --> F[‚ö° Start Orchestrator.start()]
    
    F --> G[üìÅ Phase 0: Shared Setup]
    G --> H[üîß SharedSetupService.start()]
    H --> I{üìÇ Directories OK?}
    I -->|‚ùå No| Z2[üí• Setup Failed]
    I -->|‚úÖ Yes| J[üìã Phase 1: Core Infrastructure]
    
    J --> K{üåê NGINX Enabled?}
    K -->|‚úÖ Yes| L[üåê Start NGINX Service]
    K -->|‚ùå No| M[‚è≠Ô∏è Skip NGINX]
    L --> M
    
    M --> N[üéÆ Phase 2: AI Services - Multi-GPU]
    N --> O[üî¢ Read GPU Count from Config]
    O --> P[üîÑ For Each GPU (0 to count-1)]
    
    P --> Q{üé® ComfyUI Enabled?}
    Q -->|‚úÖ Yes| R[üé® Start ComfyUI Service]
    Q -->|‚ùå No| S[‚è≠Ô∏è Skip ComfyUI]
    
    R --> R1[üìÅ Setup ComfyUI Directories]
    R1 --> R2[üìù Create Log Files]
    R2 --> R3[üîç Validate Working Directory]
    R3 --> R4[üöÄ Start Python Process]
    R4 --> R5[‚è≥ Wait for HTTP Ready]
    R5 --> R6{üè• Health Check Pass?}
    R6 -->|‚ùå No| Z3[üí• ComfyUI Failed]
    R6 -->|‚úÖ Yes| S
    
    S --> T{ü§ñ A1111 Enabled?}
    T -->|‚úÖ Yes| U[ü§ñ Start A1111 Service]
    T -->|‚ùå No| V[‚è≠Ô∏è Skip A1111]
    U --> V
    
    V --> W{üì° Redis Worker Enabled?}
    W -->|‚úÖ Yes| X[üì° Start Redis Worker]
    W -->|‚ùå No| Y[‚è≠Ô∏è Skip Worker]
    
    X --> X1[üì¶ Download Worker Package]
    X1 --> X2[üîß Extract & Setup]
    X2 --> X3[‚öôÔ∏è Create Environment File]
    X3 --> X4[üöÄ Start Worker Process]
    X4 --> Y
    
    Y --> AA{üîÑ More GPUs?}
    AA -->|‚úÖ Yes| P
    AA -->|‚ùå No| BB[üéØ Phase 3: Supporting Services]
    
    BB --> CC{üß† Ollama Enabled?}
    CC -->|‚úÖ Yes| DD[üß† Start Ollama Service]
    CC -->|‚ùå No| EE[‚è≠Ô∏è Skip Ollama]
    DD --> EE
    
    EE --> FF[‚úÖ All Services Started]
    FF --> GG[üìä Calculate Startup Time]
    GG --> HH[üéâ Emit 'ready' Event]
    HH --> II[üè• Start Health Server (Port 9090)]
    II --> JJ[üéØ Process Ready - Main Loop]
    
    JJ --> KK[üîÑ Health Monitoring Loop]
    KK --> LL[üìä Service Status Checks]
    LL --> MM[üåê HTTP Health Endpoints]
    MM --> KK
    
    %% Error Handlers
    Z1 --> ZZ[üõë Process Exit 1]
    Z2 --> ZZ
    Z3 --> ZZ
    
    %% Health Check Details
    R5 --> R5A[üåê HTTP Request to localhost:port]
    R5A --> R5B{üìä Status 200?}
    R5B -->|‚úÖ Yes| R6
    R5B -->|‚ùå No| R5C[‚è±Ô∏è Wait 1 Second]
    R5C --> R5D{‚è∞ Timeout?}
    R5D -->|‚ùå No| R5A
    R5D -->|‚úÖ Yes| Z3
```

## üîÑ Service Lifecycle Detail

```mermaid
flowchart TD
    A[üöÄ Service.start()] --> B[üìä Status: STARTING]
    B --> C[üîß onStart() Implementation]
    
    C --> D[üìÅ Setup Directories]
    D --> E[üìù Setup Logs]
    E --> F[üîç Validate Prerequisites]
    F --> G[üöÄ Start Process]
    G --> H[üíæ Save PID File]
    H --> I[‚è≥ Wait for Ready]
    
    I --> J{üè• Health Check}
    J -->|‚úÖ Pass| K[üìä Status: RUNNING]
    J -->|‚ùå Fail| L[‚è±Ô∏è Retry]
    L --> M{‚è∞ Timeout?}
    M -->|‚ùå No| J
    M -->|‚úÖ Yes| N[üí• Status: ERROR]
    
    K --> O[üéâ Emit 'started' Event]
    O --> P[üîÑ Running State]
    
    %% Shutdown Flow
    P --> Q[üõë Service.stop()]
    Q --> R[üìä Status: STOPPING]
    R --> S[üîß onStop() Implementation]
    S --> T[üîç Find Process by Port]
    T --> U[üíÄ Kill Process (SIGTERM)]
    U --> V[‚è±Ô∏è Wait 2 Seconds]
    V --> W{üîç Still Running?}
    W -->|‚úÖ Yes| X[üíÄ Force Kill (SIGKILL)]
    W -->|‚ùå No| Y[üßπ Cleanup Files]
    X --> Y
    Y --> Z[üìä Status: STOPPED]
    Z --> AA[üéâ Emit 'stopped' Event]
```

## üìä Log Output Timeline

### **1. Initial Startup (0-1s)**
```
11:23:59.845 [info] [config] Configuration loaded successfully
11:23:59.846 [info] [main] Starting Basic Machine...
11:23:59.846 [info] [orchestrator] Starting Basic Machine orchestrator...
```

### **2. Phase 0: Shared Setup (1-2s)**
```
11:23:59.847 [info] [orchestrator] Phase 0: Setting up shared directories
11:23:59.847 [info] [shared-setup] Starting service...
11:23:59.848 [info] [shared-setup] Service started successfully
```

### **3. Phase 1: Core Infrastructure (2s)**
```
11:23:59.849 [info] [orchestrator] Phase 1: Starting core infrastructure services
11:23:59.849 [info] [orchestrator] NGINX disabled, skipping
```

### **4. Phase 2: AI Services (2-4s)**
```
11:23:59.850 [info] [orchestrator] Phase 2: Starting AI services for 1 GPUs
11:23:59.850 [info] [orchestrator] Starting services for GPU 0
11:23:59.850 [info] [orchestrator]   - ComfyUI for GPU 0
11:23:59.851 [info] [comfyui] [GPU0] Starting service...
11:23:59.851 [info] [comfyui] [GPU0] Starting ComfyUI service for GPU 0
11:23:59.855 [info] [comfyui] [GPU0] Starting ComfyUI with command: python main.py...
11:24:00.916 [info] [comfyui] [GPU0] Service is ready on port 8188
11:24:00.916 [info] [comfyui] [GPU0] Service started successfully
```

### **5. Phase 3: Supporting Services (4s)**
```
11:24:00.917 [info] [orchestrator] Phase 3: Starting supporting services
11:24:00.917 [info] [orchestrator] Ollama disabled, skipping
```

### **6. Startup Complete (4-5s)**
```
11:24:00.918 [info] [orchestrator] All services started successfully in 1068ms
11:24:00.918 [info] [orchestrator] Basic Machine is ready
11:24:00.919 [info] [main] Health check server listening on port 9090
```

## üéØ Key Monitoring Points

### **Configuration**
- ‚úÖ Environment variables loaded
- ‚úÖ Service enablement flags processed
- ‚úÖ GPU count detected/configured

### **Service Health**
- ‚úÖ ComfyUI HTTP endpoint responding (port 8188)
- ‚úÖ Redis Worker connected to hub
- ‚úÖ Health monitoring active (port 9090)

### **Resource Status**
- ‚úÖ Directories created and permissions set
- ‚úÖ Log files created and writable
- ‚úÖ Processes running with correct PIDs
- ‚úÖ Ports listening and accessible

## üîç Debugging Commands

```bash
# Check service status
curl http://localhost:9090/status

# Check health
curl http://localhost:9090/health

# Check ComfyUI
curl http://localhost:8188/

# Check processes
ps aux | grep -E "(python|node)" | grep -v grep

# Check ports
netstat -tuln | grep -E "(8188|9090)"
```

## üåê Port Testing and Monitoring

### **Quick Port Tests (from root directory)**
```bash
# Check health status
pnpm machines:basic:status

# Check ComfyUI status
pnpm machines:basic:comfyui

# Check all port status
pnpm machines:basic:ports

# Follow logs
pnpm machines:basic:logs
```

### **Comprehensive Port Testing Script**
```bash
# Test all exposed ports and services
./apps/machines/basic_machine/scripts/test-ports.sh

# Expected output:
# üåê Basic Machine Port Status Check
# ==================================
# 
# üîç Testing Core Ports:
#   ‚úÖ Port 9090 (Health Monitoring) - OPEN
#   ‚úÖ Port 8188 (ComfyUI GPU0) - OPEN
# 
# üåê Testing HTTP Endpoints:
#   ‚úÖ HTTP 9090/health (Health Check) - RESPONDING
#   ‚úÖ HTTP 8188/ (ComfyUI Web UI) - RESPONDING
```

### **Exposed Ports Summary**
| Port | Service | Status | URL |
|------|---------|---------|-----|
| 22 | SSH | Always | `ssh://localhost:22` |
| 80 | HTTP (NGINX) | Optional | `http://localhost:80` |
| 443 | HTTPS (NGINX) | Optional | `https://localhost:443` |
| 8188 | ComfyUI GPU0 | Default | `http://localhost:8188` |
| 8189-8195 | ComfyUI GPU1-7 | Multi-GPU | `http://localhost:8189` |
| 9090 | Health Monitoring | Always | `http://localhost:9090/health` |
| 3001-3008 | Automatic1111 | Optional | `http://localhost:3001` |
| 11434 | Ollama | Optional | `http://localhost:11434` |

## üõ†Ô∏è Log Monitoring Script

Use the provided log monitoring script for easy log following:

```bash
# Follow all basic-machine logs
./scripts/watch-logs.sh all

# Follow ComfyUI service logs
./scripts/watch-logs.sh comfyui

# Follow Redis Worker logs
./scripts/watch-logs.sh worker

# Follow Docker container logs
./scripts/watch-logs.sh docker

# Start with debug logging
./scripts/watch-logs.sh debug
```

## üö® Common Issues and Solutions

### **Service Won't Start**
```bash
# Check configuration
LOG_LEVEL=debug npm run dev

# Check port conflicts
netstat -tuln | grep 8188

# Check directory permissions
ls -la /workspace/
```

### **Health Check Failing**
```bash
# Test health endpoint
curl -v http://localhost:9090/health

# Check service status
curl -v http://localhost:9090/status
```

### **ComfyUI Not Responding**
```bash
# Check ComfyUI logs
tail -f /workspace/comfyui_gpu0/logs/output.log

# Test ComfyUI directly
curl -v http://localhost:8188/

# Check process
ps aux | grep python | grep 8188
```

This comprehensive guide covers all aspects of monitoring and debugging the basic_machine startup process!