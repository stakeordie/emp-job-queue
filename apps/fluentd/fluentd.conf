# Fluentd Configuration for EMP Job Queue Log Aggregation
# Receives logs from Fluent Bit instances and forwards to Dash0

# System configuration
<system>
  log_level info
  suppress_repeated_stacktrace true
  emit_error_log_interval 30s
  suppress_config_dump
  without_source
  process_name fluentd-aggregator
</system>

# Input: Fluent Forward Protocol (from Fluent Bit instances)
<source>
  @type forward
  @id forward_input
  port "#{ENV['FLUENTD_FORWARD_PORT'] || '24224'}"
  bind 0.0.0.0
  
  # Security settings
  <security>
    self_hostname "#{ENV['HOSTNAME'] || 'fluentd-central'}"
    shared_key "#{ENV['FLUENTD_SHARED_KEY'] || 'emp-log-aggregation-key'}"
  </security>
  
  # Performance tuning
  chunk_size_limit 8m
  skip_invalid_event true
  resolve_hostname false
</source>

# Input: Fluent Forward with TLS (for production)
<source>
  @type forward
  @id forward_secure_input
  port "#{ENV['FLUENTD_FORWARD_TLS_PORT'] || '24225'}"
  bind 0.0.0.0
  
  <transport tls>
    cert_path "#{ENV['TLS_CERT_PATH'] || '/fluentd/certs/server.crt'}"
    private_key_path "#{ENV['TLS_KEY_PATH'] || '/fluentd/certs/server.key'}"
    ca_path "#{ENV['TLS_CA_PATH']}"
  </transport>
  
  <security>
    self_hostname "#{ENV['HOSTNAME'] || 'fluentd-central'}"
    shared_key "#{ENV['FLUENTD_SHARED_KEY'] || 'emp-log-aggregation-key'}"
  </security>
</source>

# Input: HTTP for direct log submission (development/fallback)
<source>
  @type http
  @id http_input
  port "#{ENV['FLUENTD_HTTP_PORT'] || '8888'}"
  bind 0.0.0.0
  body_size_limit 32m
  keepalive_timeout 10s
  
  # CORS for web clients
  add_http_headers true
  cors_allow_origins ["*"]
  cors_allow_methods ["POST", "PUT"]
  cors_allow_headers ["Content-Type", "Authorization", "X-Trace-ID", "X-Machine-ID"]
</source>

# Input: Health check and monitoring
<source>
  @type monitor_agent
  @id monitor_agent_input
  port "#{ENV['FLUENTD_MONITOR_PORT'] || '24220'}"
  bind 0.0.0.0
  include_tag_metrics true
  include_time_metrics true
</source>

# Input: Prometheus metrics
<source>
  @type prometheus
  @id prometheus_input
  port "#{ENV['FLUENTD_PROMETHEUS_PORT'] || '9880'}"
  bind 0.0.0.0
  metrics_path /metrics
</source>

# Filter: Add common fields to all logs
<filter **>
  @type record_transformer
  @id add_common_fields
  
  <record>
    # Environment context
    environment "#{ENV['NODE_ENV'] || 'production'}"
    service_name "#{ENV['SERVICE_NAME'] || 'emp-job-queue'}"
    region "#{ENV['AWS_REGION'] || 'unknown'}"
    
    # Processing metadata
    fluentd_host "#{ENV['HOSTNAME'] || 'fluentd-central'}"
    processed_at ${time}
    
    # Ensure correlation IDs are preserved
    trace_id ${record["trace_id"] || record["traceId"] || "unknown"}
    job_id ${record["job_id"] || record["jobId"] || "unknown"}
    machine_id ${record["machine_id"] || record["machineId"] || "unknown"}
  </record>
  
  # Remove empty fields
  remove_keys trace_id,job_id,machine_id
  renew_record true
</filter>

# Filter: Parse and enhance log messages
<filter **>
  @type parser
  @id parse_log_messages
  
  key_name message
  reserve_data true
  reserve_time true
  remove_key_name_field false
  
  # Try multiple parsers
  <parse>
    @type multi_format
    
    # JSON logs (preferred)
    <pattern>
      format json
      time_key timestamp
      time_type string
      time_format %Y-%m-%dT%H:%M:%S.%LZ
    </pattern>
    
    # Structured text logs
    <pattern>
      format regexp
      expression /^\[(?<timestamp>[^\]]+)\] (?<level>\w+): (?<message>.*)/
      time_key timestamp
      time_type string
    </pattern>
    
    # Fall back to plain text
    <pattern>
      format none
    </pattern>
  </parse>
</filter>

# Filter: Route by log level and source
<filter **>
  @type rewrite_tag_filter
  @id route_by_context
  
  # Critical errors get special handling
  <rule>
    key level
    pattern /^(ERROR|FATAL)$/i
    tag critical.${tag}
  </rule>
  
  # Machine logs get machine-specific routing
  <rule>
    key source
    pattern /^machine\./
    tag machine.${tag}
  </rule>
  
  # Worker logs get worker-specific routing  
  <rule>
    key source
    pattern /^worker\./
    tag worker.${tag}
  </rule>
  
  # Job execution logs
  <rule>
    key context
    pattern /^job\./
    tag job.${tag}
  </rule>
  
  # Default routing
  <rule>
    key level
    pattern /.*/
    tag application.${tag}
  </rule>
</filter>

# Buffer configuration for reliability
<match **>
  @type copy
  
  # Primary output: Send to Dash0 via HTTP
  <store>
    @type http
    @id dash0_output
    
    endpoint "#{ENV['DASH0_LOGS_ENDPOINT'] || 'https://ingress.us-west-2.aws.dash0.com/logs/json'}"
    open_timeout 10
    read_timeout 30
    compress gzip
    
    # Authentication
    headers {
      "Authorization": "Bearer #{ENV['DASH0_API_KEY']}"
      "Content-Type": "application/json"
      "Dash0-Dataset": "#{ENV['DASH0_DATASET'] || ENV['NODE_ENV'] || 'development'}"
    }
    
    # Buffering for reliability
    <buffer>
      @type file
      path /fluentd/buffer/dash0
      
      # Reliability settings
      flush_mode interval
      flush_interval 5s
      flush_at_shutdown true
      retry_max_times 10
      retry_wait 1s
      retry_exponential_backoff_base 2
      retry_max_interval 60s
      
      # Performance settings  
      chunk_limit_size 8MB
      queue_limit_length 64
      overflow_action block
      
      # Persistence
      total_limit_size 256MB
      flush_thread_count 2
    </buffer>
    
    # Error handling
    <secondary>
      @type redis
      host "#{ENV['REDIS_HOST'] || 'localhost'}"
      port "#{ENV['REDIS_PORT'] || '6379'}"
      password "#{ENV['REDIS_PASSWORD']}"
      db_number 2
      key_prefix "fluentd_failed_logs"
    </secondary>
    
    # Format for Dash0
    <format>
      @type json
    </format>
  </store>
  
  # Secondary output: Local file backup
  <store>
    @type file
    @id local_backup
    path /fluentd/logs/backup.log
    
    <buffer>
      @type file
      path /fluentd/buffer/backup
      flush_mode interval  
      flush_interval 60s
      chunk_limit_size 16MB
    </buffer>
    
    <format>
      @type json
      time_key timestamp
      time_type string
      time_format %Y-%m-%dT%H:%M:%S.%LZ
    </format>
  </store>
  
  # Monitoring output: Count and metrics
  <store>
    @type prometheus
    @id prometheus_metrics
    
    <metric>
      name fluentd_input_status_num_records_total
      type counter
      desc The total number of incoming records
    </metric>
    
    <metric>
      name fluentd_output_status_num_records_total  
      type counter
      desc The total number of outgoing records
    </metric>
  </store>
</match>

# Special handling for critical errors
<match critical.**>
  @type copy
  
  # Send immediately to Dash0 with high priority
  <store>
    @type http
    @id dash0_critical
    
    endpoint "#{ENV['DASH0_LOGS_ENDPOINT'] || 'https://ingress.us-west-2.aws.dash0.com/logs/json'}"
    compress gzip
    
    headers {
      "Authorization": "Bearer #{ENV['DASH0_API_KEY']}"
      "Content-Type": "application/json"
      "Dash0-Dataset": "#{ENV['DASH0_DATASET'] || ENV['NODE_ENV'] || 'development'}"
      "X-Priority": "critical"
    }
    
    # No buffering for critical logs
    <buffer>
      flush_mode immediate
      retry_max_times 5
      retry_wait 0.5s
    </buffer>
    
    <format>
      @type json
    </format>
  </store>
  
  # Also store in Redis for immediate alerting
  <store>
    @type redis
    @id redis_critical_alerts
    
    host "#{ENV['REDIS_HOST'] || 'localhost'}"
    port "#{ENV['REDIS_PORT'] || '6379'}"
    password "#{ENV['REDIS_PASSWORD']}"
    db_number 3
    
    # Use list for FIFO processing
    command "lpush"
    key_prefix "critical_logs"
    
    <format>
      @type json
    </format>
  </store>
</match>

# Include any custom configurations
@include /fluentd/etc/custom/*.conf