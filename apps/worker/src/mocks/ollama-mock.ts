/**
 * Ollama Mock - Simulates Ollama API with realistic job progression
 */

import { BaseProgressiveMock, ProgressiveMockConfig } from './base-progressive-mock.js';

export class OllamaMock extends BaseProgressiveMock {
  constructor(baseUrl: string = 'http://localhost:11434') {
    const config: ProgressiveMockConfig = {
      baseUrl,
      submitEndpoint: '/api/generate',
      statusEndpoint: '/api/jobs/:id', // This endpoint doesn't exist in real Ollama, but we're simulating it
      progressSteps: [0, 15, 35, 60, 85, 100], // Ollama tends to be fast
      stepDelayMs: 800, // 800ms per step = ~4 seconds total
      serviceName: 'Ollama',
      finalResponse: {
        model: "qwen3:0.6b",
        created_at: new Date().toISOString(),
        response: "This is a realistic mock response from Ollama! The text generation completed successfully in our staging environment. This response includes proper formatting and realistic content that would be generated by a language model.",
        done: true,
        total_duration: 3200000000, // 3.2 seconds in nanoseconds
        load_duration: 150000000,   // 150ms load time
        prompt_eval_count: 25,
        prompt_eval_duration: 800000000,
        eval_count: 45,
        eval_duration: 2250000000,
        context: [128, 256, 512, 1024] // Mock context tokens
      }
    };

    super(config);
    this.setupOllamaSpecificEndpoints();
  }

  private setupOllamaSpecificEndpoints() {
    // Add Ollama health check endpoint
    this.addCustomEndpoint('get', '/api/version', {
      version: "0.1.0"
    });

    // Add models list endpoint
    this.addCustomEndpoint('get', '/api/tags', {
      models: [
        {
          name: "qwen3:0.6b",
          model: "qwen3:0.6b",
          size: 522000000,
          digest: "sha256:mock-digest-qwen",
          details: {
            format: "gguf",
            family: "qwen",
            families: ["qwen"],
            parameter_size: "0.6B",
            quantization_level: "Q4_K_M"
          },
          modified_at: "2025-09-25T10:00:00Z"
        },
        {
          name: "tinyllama:1.1b",
          model: "tinyllama:1.1b",
          size: 637000000,
          digest: "sha256:mock-digest-tiny",
          details: {
            format: "gguf",
            family: "llama",
            families: ["llama"],
            parameter_size: "1.1B",
            quantization_level: "Q4_K_M"
          },
          modified_at: "2025-09-25T09:30:00Z"
        }
      ]
    });

    // Add chat endpoint (different from generate)
    this.addCustomEndpoint('post', '/api/chat', {
      model: "qwen3:0.6b",
      created_at: new Date().toISOString(),
      message: {
        role: "assistant",
        content: "Mock chat response from Ollama staging environment! This simulates a conversational AI response."
      },
      done: true,
      total_duration: 2800000000,
      load_duration: 120000000,
      prompt_eval_count: 30,
      prompt_eval_duration: 900000000,
      eval_count: 35,
      eval_duration: 1780000000
    });

    // Add embeddings endpoint
    this.addCustomEndpoint('post', '/api/embeddings', {
      embedding: new Array(768).fill(0).map(() => Math.random() * 2 - 1) // Mock 768-dim embedding
    });
  }

  // Simulate Ollama-specific error conditions
  public simulateModelNotFound() {
    this.simulateError('/api/generate', 404, {
      error: "model 'non-existent-model' not found, try pulling it first"
    });
  }

  public simulateOutOfMemory() {
    this.simulateError('/api/generate', 500, {
      error: "failed to load model: not enough memory"
    });
  }
}