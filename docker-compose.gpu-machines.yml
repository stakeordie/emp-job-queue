# Production GPU Machine Deployment
# This compose file sets up GPU machines that:
# 1. Use the published emprops/emp-job-queue-api:latest for the API server
# 2. Build GPU machines locally (not published to registry yet)
# 3. Download worker bundles from GitHub releases automatically
#
# Usage:
#   docker-compose -f docker-compose.prod.yml -f docker-compose.gpu-machines.yml up

version: '3.8'

services:
  # GPU Machine 1
  gpu-machine-1:
    build:
      context: apps/machines/basic_machine
      dockerfile: Dockerfile
      args:
        CACHE_BUST: ${CACHE_BUST:-1}
        CUSTOM_NODES_CACHE_BUST: ${CUSTOM_NODES_CACHE_BUST:-default}
        # Build-time secrets for custom nodes
        AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
        AWS_SECRET_ACCESS_KEY_ENCODED: ${AWS_SECRET_ACCESS_KEY_ENCODED}
        AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
        HF_TOKEN: ${HF_TOKEN}
        CIVITAI_TOKEN: ${CIVITAI_TOKEN}
        OPENAI_API_KEY: ${OPENAI_API_KEY}
    container_name: gpu-machine-1
    hostname: gpu-machine-1
    restart: unless-stopped
    
    # GPU access (auto-discovered at runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Environment for GPU machine
    environment:
      - NODE_ENV=production
      - TEST_MODE=${TEST_MODE:-false}
      # GPU specs auto-discovered at runtime via nvidia-smi
      - CONTAINER_NAME=gpu-machine-1
      - MACHINE_ID=gpu-machine-1
      
      # Redis connection (connects to redis service in main compose)
      - HUB_REDIS_URL=redis://redis:6379
      
      # Worker configuration
      - WORKER_CONNECTORS=${WORKER_CONNECTORS:-comfyui,simulation}
      - WORKER_WEBSOCKET_AUTH_TOKEN=${WORKER_WEBSOCKET_AUTH_TOKEN}
      - WORKER_ID_PREFIX=gpu-machine-1
      
      # Service toggles
      - ENABLE_NGINX=${ENABLE_NGINX:-false}
      - ENABLE_COMFYUI=${ENABLE_COMFYUI:-true}
      - ENABLE_A1111=${ENABLE_A1111:-false}
      - ENABLE_REDIS_WORKERS=${ENABLE_REDIS_WORKERS:-true}
      - ENABLE_OLLAMA=${ENABLE_OLLAMA:-false}
      
      # Logging
      - LOG_LEVEL=${LOG_LEVEL:-info}
      
      # Ports (internal)
      - SSH_PORT=22
      - NGINX_HTTP_PORT=80
      - COMFYUI_PORT_START=8188
      - HEALTH_PORT=9090
      
      # Model management
      - SKIP_MODEL_DOWNLOAD=${SKIP_MODEL_DOWNLOAD:-false}
      - HF_TOKEN=${HF_TOKEN}
      - CIVITAI_TOKEN=${CIVITAI_TOKEN}
      
      # Cloud storage (optional)
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY_ENCODED=${AWS_SECRET_ACCESS_KEY_ENCODED}
      - CLOUD_PROVIDER=${CLOUD_PROVIDER}
      - CLOUD_MODELS_CONTAINER=${CLOUD_MODELS_CONTAINER}
      - CLOUD_STORAGE_CONTAINER=${CLOUD_STORAGE_CONTAINER}
      
      # Worker download (automatically downloads from GitHub releases)
      # No WORKER_LOCAL_PATH = downloads from GitHub releases
      # WORKER_LOCAL_PATH set = uses local bundled worker (development)
    
    # Port mapping for GPU machine 1
    ports:
      - "2230:22"      # SSH
      - "8090:80"      # HTTP
      - "3200:8188"    # ComfyUI GPU0
      - "3201:8189"    # ComfyUI GPU1 (if NUM_GPUS > 1)
      - "9100:9090"    # Health check
    
    # Persistent storage for GPU machine 1
    volumes:
      - gpu_machine_1_data:/workspace/data
      - gpu_machine_1_logs:/workspace/logs
      - gpu_machine_1_models:/workspace/models
      - gpu_machine_1_comfyui:/workspace/ComfyUI
      - gpu_machine_1_shared:/workspace/shared
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s  # Give time for ComfyUI custom nodes to install
    
    # Depend on Redis being available
    depends_on:
      redis:
        condition: service_healthy

  # GPU Machine 2 (optional, scale as needed)
  gpu-machine-2:
    build:
      context: apps/machines/basic_machine
      dockerfile: Dockerfile
      args:
        CACHE_BUST: ${CACHE_BUST:-1}
        CUSTOM_NODES_CACHE_BUST: ${CUSTOM_NODES_CACHE_BUST:-default}
        AWS_ACCESS_KEY_ID: ${AWS_ACCESS_KEY_ID}
        AWS_SECRET_ACCESS_KEY_ENCODED: ${AWS_SECRET_ACCESS_KEY_ENCODED}
        AWS_DEFAULT_REGION: ${AWS_DEFAULT_REGION}
        HF_TOKEN: ${HF_TOKEN}
        CIVITAI_TOKEN: ${CIVITAI_TOKEN}
        OPENAI_API_KEY: ${OPENAI_API_KEY}
    container_name: gpu-machine-2
    hostname: gpu-machine-2
    restart: unless-stopped
    
    # GPU access (auto-discovered at runtime)
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: all
              capabilities: [gpu]
    
    # Environment for GPU machine 2
    environment:
      - NODE_ENV=production
      - TEST_MODE=${TEST_MODE:-false}
      # GPU specs auto-discovered at runtime via nvidia-smi
      - CONTAINER_NAME=gpu-machine-2
      - MACHINE_ID=gpu-machine-2
      - HUB_REDIS_URL=redis://redis:6379
      - WORKER_CONNECTORS=${WORKER_CONNECTORS:-comfyui,simulation}
      - WORKER_WEBSOCKET_AUTH_TOKEN=${WORKER_WEBSOCKET_AUTH_TOKEN}
      - WORKER_ID_PREFIX=gpu-machine-2
      - ENABLE_NGINX=${ENABLE_NGINX:-false}
      - ENABLE_COMFYUI=${ENABLE_COMFYUI:-true}
      - ENABLE_A1111=${ENABLE_A1111:-false}
      - ENABLE_REDIS_WORKERS=${ENABLE_REDIS_WORKERS:-true}
      - ENABLE_OLLAMA=${ENABLE_OLLAMA:-false}
      - LOG_LEVEL=${LOG_LEVEL:-info}
      - SSH_PORT=22
      - NGINX_HTTP_PORT=80
      - COMFYUI_PORT_START=8188
      - HEALTH_PORT=9090
      - SKIP_MODEL_DOWNLOAD=${SKIP_MODEL_DOWNLOAD:-false}
      - HF_TOKEN=${HF_TOKEN}
      - CIVITAI_TOKEN=${CIVITAI_TOKEN}
      - AWS_DEFAULT_REGION=${AWS_DEFAULT_REGION}
      - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
      - AWS_SECRET_ACCESS_KEY_ENCODED=${AWS_SECRET_ACCESS_KEY_ENCODED}
      - CLOUD_PROVIDER=${CLOUD_PROVIDER}
      - CLOUD_MODELS_CONTAINER=${CLOUD_MODELS_CONTAINER}
      - CLOUD_STORAGE_CONTAINER=${CLOUD_STORAGE_CONTAINER}
    
    # Port mapping for GPU machine 2 (different ports to avoid conflicts)
    ports:
      - "2231:22"      # SSH
      - "8091:80"      # HTTP  
      - "3210:8188"    # ComfyUI GPU0
      - "3211:8189"    # ComfyUI GPU1 (if NUM_GPUS > 1)
      - "9101:9090"    # Health check
    
    # Persistent storage for GPU machine 2
    volumes:
      - gpu_machine_2_data:/workspace/data
      - gpu_machine_2_logs:/workspace/logs
      - gpu_machine_2_models:/workspace/models
      - gpu_machine_2_comfyui:/workspace/ComfyUI
      - gpu_machine_2_shared:/workspace/shared
    
    # Health check
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9090/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s
    
    # Depend on Redis being available
    depends_on:
      redis:
        condition: service_healthy
    
    # Scale this service by commenting out or using profiles
    profiles:
      - multi-gpu

# Named volumes for persistent storage
volumes:
  # GPU Machine 1 volumes
  gpu_machine_1_data:
    driver: local
  gpu_machine_1_logs:
    driver: local
  gpu_machine_1_models:
    driver: local
  gpu_machine_1_comfyui:
    driver: local
  gpu_machine_1_shared:
    driver: local
  
  # GPU Machine 2 volumes  
  gpu_machine_2_data:
    driver: local
  gpu_machine_2_logs:
    driver: local
  gpu_machine_2_models:
    driver: local
  gpu_machine_2_comfyui:
    driver: local
  gpu_machine_2_shared:
    driver: local